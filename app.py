import streamlit as st
import cv2
import numpy as np
from PIL import Image
import io
import requests
import os
import zipfile
from deepface import DeepFace
import tempfile # Th∆∞ vi·ªán m·ªõi ƒë·ªÉ t·∫°o file t·∫°m duy nh·∫•t
import time 

# --- 1. Thi·∫øt l·∫≠p trang Streamlit ---
st.set_page_config(
    page_title="H·ªá th·ªëng ƒêi·ªÉm danh Khu√¥n m·∫∑t DeepFace (GDrive)",
    page_icon="üì∏",
    layout="centered"
)

st.title("üì∏ H·ªá th·ªëng ƒêi·ªÉm danh Khu√¥n m·∫∑t DeepFace")
st.caption("Dataset ƒë∆∞·ª£c t·∫£i t·ª´ Google Drive c√¥ng khai qua file ZIP.")

# --- 2. C·∫•u h√¨nh & H·∫±ng s·ªë ---
HAAR_CASCADE_URL = "https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
CASCADE_FILENAME = 'haarcascade_frontalface_default.xml'

# Vui l√≤ng thay th·∫ø chu·ªói n√†y b·∫±ng File ID c·ªßa file ZIP dataset c√¥ng khai c·ªßa b·∫°n.
# VD: GDRIVE_FILE_ID = "1a2b3c4d5e6f7g8h9i0j"
GDRIVE_FILE_ID = "YOUR_GDRIVE_FILE_ID_HERE" 
ZIP_FILENAME = "dataset_archive.zip" 
DATASET_FOLDER = "dataset" 
# S·ª≠ d·ª•ng detector_backend="opencv" ƒë·ªÉ tr√°nh l·ªói TypeError/Keras/TensorFlow
DETECTOR_BACKEND = "opencv"


@st.cache_resource
def load_face_cascade(url, filename):
    """ T·∫£i Haar Cascade cho OpenCV. """
    try:
        r = requests.get(url)
        if r.status_code == 200:
            with open(filename, 'wb') as f:
                f.write(r.content)
            classifier = cv2.CascadeClassifier(filename)
            if not classifier.empty():
                return classifier
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i ho·∫∑c kh·ªüi t·∫°o Haar Cascade: {e}")
        return None

face_cascade = load_face_cascade(HAAR_CASCADE_URL, CASCADE_FILENAME)


@st.cache_resource(show_spinner="ƒêang t·∫£i v√† gi·∫£i n√©n Dataset t·ª´ Google Drive (Ch·ªâ ch·∫°y l·∫ßn ƒë·∫ßu)...")
def download_and_extract_dataset(file_id, zip_name, target_folder):
    """
    T·∫£i file ZIP c√¥ng khai t·ª´ Google Drive v√† gi·∫£i n√©n.
    """
    if not file_id or file_id == "YOUR_GDRIVE_FILE_ID_HERE":
        st.error("‚ùå Vui l√≤ng thay th·∫ø 'YOUR_GDRIVE_FILE_ID_HERE' b·∫±ng File ID th·ª±c t·∫ø c·ªßa file ZIP dataset.")
        return False
        
    # Ki·ªÉm tra nhanh: N·∫øu th∆∞ m·ª•c dataset t·ªìn t·∫°i v√† ƒë√£ c√≥ cache DeepFace (ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥)
    deepface_cache = os.path.join(target_folder, 'representations_arcface.pkl')
    if os.path.exists(deepface_cache) and os.path.isdir(target_folder) and len(os.listdir(target_folder)) > 1:
         st.success(f"Dataset ƒë√£ s·∫µn s√†ng t·∫°i '{target_folder}'. B·ªè qua t·∫£i xu·ªëng.")
         return True
    
    st.info(f"ƒêang t·∫£i dataset t·ª´ Google Drive File ID: {file_id}...")
    DOWNLOAD_URL = f"https://drive.google.com/uc?export=download&id={file_id}"
    
    try:
        # T·∫£i xu·ªëng file ZIP
        response = requests.get(DOWNLOAD_URL, stream=True)
        response.raise_for_status() 
        
        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p Google Drive c·∫£nh b√°o file l·ªõn
        if "confirm" in response.headers.get("Content-Disposition", ""):
            st.warning("Google Drive ƒëang y√™u c·∫ßu x√°c nh·∫≠n t·∫£i file l·ªõn. ƒêang th·ª≠ t·∫£i l·∫°i.")
            for key, value in response.cookies.items():
                if key.startswith('download_warning'):
                    params = {'id': file_id, 'confirm': value}
                    response = requests.get(DOWNLOAD_URL, params=params, stream=True)
                    response.raise_for_status()
                    break

        # L∆∞u file zip
        with open(zip_name, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        
        # Gi·∫£i n√©n
        with zipfile.ZipFile(zip_name, 'r') as zip_ref:
            if not os.path.exists(target_folder):
                os.makedirs(target_folder)
            # DeepFace y√™u c·∫ßu dataset folder ph·∫£i n·∫±m ngay trong th∆∞ m·ª•c g·ªëc
            zip_ref.extractall(".")
            
        st.success(f"Gi·∫£i n√©n th√†nh c√¥ng v√†o th∆∞ m·ª•c '{target_folder}'.")
        
        # X√≥a file zip t·∫°m
        if os.path.exists(zip_name):
            os.remove(zip_name)
        
        return True

    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i xu·ªëng ho·∫∑c gi·∫£i n√©n dataset t·ª´ Drive: {e}. Vui l√≤ng ki·ªÉm tra File ID v√† quy·ªÅn chia s·∫ª c√¥ng khai.")
        if os.path.exists(zip_name):
            os.remove(zip_name)
        return False


# --- 3. H√†m Ph√°t hi·ªán Khu√¥n m·∫∑t (D√πng cho hi·ªÉn th·ªã khung) ---
def detect_and_draw_face(image_bytes, cascade):
    """
    D√πng Haar Cascade ƒë·ªÉ ph√°t hi·ªán v√† v·∫Ω khung khu√¥n m·∫∑t tr√™n ·∫£nh.
    """
    image_pil = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    image_np = np.array(image_pil)
    image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    
    faces = []
    if cascade is not None:
        faces = cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # V·∫Ω khung vu√¥ng l√™n ·∫£nh
    for (x, y, w, h) in faces:
        cv2.rectangle(image_bgr, (x, y), (x + w, y + h), (255, 0, 0), 2)
    
    processed_image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

    return processed_image_rgb, len(faces) > 0, len(faces), image_bgr


# --- 4. H√†m DeepFace Recognition (S·ª≠ d·ª•ng detector_backend="opencv") ---
def verify_face_against_dataset(target_image_path, dataset_folder):
    """
    S·ª≠ d·ª•ng DeepFace ƒë·ªÉ so s√°nh ·∫£nh ƒë·∫ßu v√†o v·ªõi dataset.
    """
    try:
        # THAY ƒê·ªîI QUAN TR·ªåNG: S·ª≠ d·ª•ng detector_backend="opencv"
        df_list = DeepFace.find(
            img_path=target_image_path, 
            db_path=dataset_folder, 
            model_name="ArcFace",
            distance_metric="cosine",
            enforce_detection=True, 
            detector_backend=DETECTOR_BACKEND 
        )
        
        if isinstance(df_list, list) and len(df_list) > 0 and not df_list[0].empty:
            best_match = df_list[0].iloc[0]
            identity_path = best_match['identity']
            # L·∫•y t√™n ng∆∞·ªùi t·ª´ t√™n file (lo·∫°i b·ªè ph·∫ßn m·ªü r·ªông)
            person_name = os.path.splitext(os.path.basename(identity_path))[0] 
            distance = best_match['ArcFace_cosine'] 
            return person_name, distance
        
        return None, None
    
    except ValueError as e:
        if "Face could not be detected" in str(e):
             # DeepFace.find() s·∫Ω n√©m ValueError n·∫øu kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t
             st.error(f"‚ùå L·ªói DeepFace: Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t ƒë·ªÉ so kh·ªõp. Vui l√≤ng th·ª≠ l·∫°i ·∫£nh r√µ r√†ng h∆°n.")
        else:
            st.error(f"‚ùå L·ªói DeepFace: {e}")
        return None, None
    except Exception as e:
        st.error(f"‚ùå L·ªói trong qu√° tr√¨nh so kh·ªõp DeepFace: {e}")
        return None, None


# --- 5. Giao di·ªán v√† Lu·ªìng ·ª®ng d·ª•ng ---

# 5.1 KH·ªûI T·∫†O V√Ä T·∫¢I DATASET (Ch·∫°y ƒë·∫ßu ti√™n)
dataset_ready = download_and_extract_dataset(GDRIVE_FILE_ID, ZIP_FILENAME, DATASET_FOLDER)

st.markdown("---")

if not dataset_ready:
     st.warning("‚ö†Ô∏è Vui l√≤ng c·∫•u h√¨nh ƒë√∫ng File ID ZIP c√¥ng khai v√† th·ª≠ l·∫°i.")
     st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu dataset ch∆∞a s·∫µn s√†ng

st.info(f"Dataset ƒë√£ t·∫£i xong. DeepFace s·∫Ω s·ª≠ d·ª•ng detector: **{DETECTOR_BACKEND.upper()}**.")


# 5.2 CH·ª§P ·∫¢NH V√Ä X·ª¨ L√ù
captured_file = st.camera_input("Ch·ª•p ·∫£nh ƒëi·ªÉm danh:")

if captured_file is not None:
    if face_cascade is None:
        st.error("Kh√¥ng th·ªÉ ti·∫øp t·ª•c do l·ªói t·∫£i b·ªô ph√¢n lo·∫°i khu√¥n m·∫∑t.")
    else:
        image_bytes = captured_file.getvalue()
        
        # M·ªü spinner trong l√∫c x·ª≠ l√Ω
        with st.spinner('ƒêang x·ª≠ l√Ω ·∫£nh v√† nh·∫≠n di·ªán khu√¥n m·∫∑t...'):
            
            # 1. Ph√°t hi·ªán khu√¥n m·∫∑t v√† v·∫Ω khung (D√πng cho hi·ªÉn th·ªã)
            processed_image_np, face_detected, num_faces, image_bgr = detect_and_draw_face(image_bytes, face_cascade)
            processed_image = Image.fromarray(processed_image_np)
            
            # 2. L∆ØU ·∫¢NH T·∫†M TH·ªúI DUY NH·∫§T (QUAN TR·ªåNG: D√πng tempfile)
            temp_file = tempfile.NamedTemporaryFile(suffix=".jpg", delete=False)
            TEMP_IMAGE_PATH = temp_file.name
            temp_file.close() # ƒê√≥ng file handle ƒë·ªÉ cv2.imwrite c√≥ th·ªÉ ghi v√†o
            
            cv2.imwrite(TEMP_IMAGE_PATH, image_bgr)
            
            # 3. Th·ª±c hi·ªán so kh·ªõp DeepFace
            match_name, distance = verify_face_against_dataset(TEMP_IMAGE_PATH, DATASET_FOLDER)

        # X√≥a file t·∫°m sau khi ƒë√£ x·ª≠ l√Ω xong
        if os.path.exists(TEMP_IMAGE_PATH):
            os.remove(TEMP_IMAGE_PATH)
            
        st.subheader("üñºÔ∏è ·∫¢nh ƒë√£ ch·ª•p v√† Nh·∫≠n di·ªán")
        st.image(processed_image, caption="Khu√¥n m·∫∑t ƒë√£ ph√°t hi·ªán ƒë∆∞·ª£c ƒë√°nh d·∫•u b·∫±ng khung m√†u xanh d∆∞∆°ng (OpenCV).", use_column_width=True)

        st.markdown("---")
        st.subheader("üí° K·∫øt qu·∫£ ƒêi·ªÉm danh")
        
        if match_name:
            st.balloons()
            st.success(f"‚úÖ **ƒêI·ªÇM DANH TH√ÄNH C√îNG!**")
            st.markdown(f"""
            * **Ng∆∞·ªùi tr√πng kh·ªõp:** **{match_name}**
            * **ƒê·ªô t∆∞∆°ng ƒë·ªìng (Kho·∫£ng c√°ch Cosine):** `{distance:.4f}`
            """)
            
        elif face_detected and num_faces > 0:
            st.warning(f"‚ö†Ô∏è **Ph√°t hi·ªán {num_faces} khu√¥n m·∫∑t, nh∆∞ng kh√¥ng kh·ªõp v·ªõi dataset.**")
            st.markdown("""
            * **G·ª£i √Ω:** Khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán, nh∆∞ng kh√¥ng ƒë·ªß ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi b·∫•t k·ª≥ ng∆∞·ªùi n√†o trong dataset.
            """)
            
        else:
            st.warning("‚ö†Ô∏è **Kh√¥ng ph√°t hi·ªán th·∫•y khu√¥n m·∫∑t.**")
            st.markdown("Vui l√≤ng th·ª≠ l·∫°i. ƒê·∫£m b·∫£o khu√¥n m·∫∑t c·ªßa b·∫°n n·∫±m g·ªçn v√† r√µ r√†ng trong khung h√¨nh.")
