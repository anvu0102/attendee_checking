import streamlit as st
import cv2
import numpy as np
from PIL import Image
import io
import requests
import os
import zipfile
from deepface import DeepFace
import tempfile
import time 
import pandas as pd

# --- 1. Thi·∫øt l·∫≠p trang Streamlit ---
st.set_page_config(
    page_title="H·ªá th·ªëng ƒêi·ªÉm danh Khu√¥n m·∫∑t DeepFace (GDrive)",
    page_icon="üì∏",
    layout="centered"
)

st.title("üì∏ H·ªá th·ªëng ƒêi·ªÉm danh Khu√¥n m·∫∑t DeepFace")
st.caption("S·ª≠ d·ª•ng ID Drive v√† OAuth Credentials t·ª´ st.secrets.")

# --- 2. C·∫•u h√¨nh & H·∫±ng s·ªë (T·∫¢I T·ª™ ST.SECRETS) ---
HAAR_CASCADE_URL = "https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
CASCADE_FILENAME = 'haarcascade_frontalface_default.xml'

# T·∫¢I C√ÅC TH√îNG TIN T·ª™ ST.SECRETS
# ƒê·∫£m b·∫£o c√°c kh√≥a n√†y ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong file secrets.toml
try:
    GDRIVE_CLIENT_ID = st.secrets["GDRIVE_CLIENT_ID"]
    GDRIVE_CLIENT_SECRET = st.secrets["GDRIVE_CLIENT_SECRET"]
    GDRIVE_DATASET_FOLDER_ID = st.secrets["GDRIVE_DATASET_ID"] 
    GDRIVE_CHECKLIST_ID = st.secrets["GDRIVE_CHECKLIST_ID"]
    GDRIVE_NEW_DATA_FOLDER_ID = st.secrets["GDRIVE_NEW_DATA_ID"]
except KeyError as e:
    st.error(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y kh√≥a {e} trong st.secrets.")
    st.info("Vui l√≤ng ƒë·∫£m b·∫£o b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a t·∫•t c·∫£ c√°c kh√≥a (CLIENT_ID, CLIENT_SECRET, DATASET_ID, CHECKLIST_ID, NEW_DATA_ID) trong file .streamlit/secrets.toml ho·∫∑c trong giao di·ªán Secrets c·ªßa Streamlit Cloud.")
    st.stop()

# C√°c h·∫±ng s·ªë kh√°c
DATASET_FOLDER = "dataset" 
CHECKLIST_FILENAME = "checklist.xlsx" 
CHECKLIST_SESSION_KEY = "attendance_df" 
DETECTOR_BACKEND = "opencv"


# --- H√†m Gi·∫£ L·∫≠p X√°c Th·ª±c Token ---
def get_valid_access_token_mock(client_id, client_secret):
    """ 
    [MOCK] Gi·∫£ l·∫≠p quy tr√¨nh OAuth 2.0 ƒë·ªÉ l·∫•y Access Token.
    Trong th·ª±c t·∫ø, h√†m n√†y s·∫Ω s·ª≠ d·ª•ng Client ID/Secret ƒë·ªÉ y√™u c·∫ßu v√† l√†m m·ªõi token.
    """
    if client_id.startswith("YOUR_OAUTH"):
        st.error("‚ùå L·ªói c·∫•u h√¨nh: Client ID v·∫´n l√† placeholder. Kh√¥ng th·ªÉ gi·∫£ l·∫≠p token.")
        return None
    
    st.success("‚úÖ Gi·∫£ l·∫≠p: ƒê√£ s·ª≠ d·ª•ng Client ID/Secret ƒë·ªÉ t·∫°o Access Token (Token th·ª±c t·∫ø c·∫ßn lu·ªìng OAuth).")
    # Gi·∫£ l·∫≠p tr·∫£ v·ªÅ m·ªôt Token
    return "MOCK_ACCESS_TOKEN_" + client_id[:5] 


# --- H√†m t·∫£i file ƒë∆°n l·∫ª t·ª´ Google Drive (D√πng cho Checklist XLSX) ---
def download_file_from_gdrive(file_id, output_filename, access_token=None):
    """ T·∫£i file t·ª´ Google Drive. C·∫ßn Access Token cho c√°c file kh√¥ng c√¥ng khai. """
    DOWNLOAD_URL = f"https://drive.google.com/uc?export=download&id={file_id}"
    
    headers = {}
    if access_token:
        # N·∫øu d√πng token, ph·∫£i th√™m v√†o Header
        headers = {'Authorization': f'Bearer {access_token}'} 
    
    try:
        response = requests.get(DOWNLOAD_URL, stream=True, headers=headers)
        response.raise_for_status() 
        
        if "confirm" in response.headers.get("Content-Disposition", ""):
            params = {'id': file_id, 'confirm': 't'}
            response = requests.get(DOWNLOAD_URL, params=params, stream=True, headers=headers)
            response.raise_for_status()

        with open(output_filename, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        return True
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i file {output_filename} t·ª´ Drive: {e}")
        st.warning("G·ª£i √Ω: Ki·ªÉm tra ID file, quy·ªÅn chia s·∫ª v√† Access Token.")
        return False


# --- MOCK: T·∫£i Folder Dataset (M√¥ ph·ªèng) ---
@st.cache_resource(show_spinner="ƒêang m√¥ ph·ªèng t·∫£i Dataset FOLDER t·ª´ Google Drive...")
def download_dataset_folder_mock(folder_id, target_folder, access_token):
    """ MOCK: M√¥ ph·ªèng t·∫£i to√†n b·ªô n·ªôi dung folder Drive v√†o th∆∞ m·ª•c local, s·ª≠ d·ª•ng token. """
    st.warning("‚ö†Ô∏è CH√ö √ù: H√†m n√†y ch·ªâ MOCK (gi·∫£ l·∫≠p). C·∫ßn Google Drive API th·ª±c t·∫ø v√† Access Token h·ª£p l·ªá.")
    st.info(f"Gi·∫£ l·∫≠p: S·ª≠ d·ª•ng Access Token ƒë·ªÉ truy c·∫≠p Folder ID: {folder_id}.")

    if not os.path.exists(target_folder):
        os.makedirs(target_folder)
        try:
            # T·∫°o c·∫•u tr√∫c file gi·∫£ ƒë·ªãnh ƒë·ªÉ DeepFace c√≥ th·ªÉ ch·∫°y
            temp_img1 = np.zeros((100, 100, 3), dtype=np.uint8)
            temp_img2 = np.zeros((100, 100, 3), dtype=np.uint8)
            cv2.imwrite(os.path.join(target_folder, "1.jpg"), temp_img1) 
            cv2.imwrite(os.path.join(target_folder, "2.jpg"), temp_img2) 
            st.success(f"M√¥ ph·ªèng: ƒê√£ t·∫°o th∆∞ m·ª•c '{target_folder}' v·ªõi c√°c file m·∫´u. S·∫µn s√†ng cho DeepFace.")
            return True
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o file mock: {e}")
            return False

    deepface_cache = os.path.join(target_folder, 'representations_arcface.pkl')
    if os.path.isdir(target_folder) and (len(os.listdir(target_folder)) > 2 or os.path.exists(deepface_cache)):
         st.success(f"Dataset folder ƒë√£ s·∫µn s√†ng t·∫°i '{target_folder}'. B·ªè qua t·∫£i xu·ªëng.")
         return True
    
    return False


@st.cache_data(show_spinner="ƒêang t·∫£i v√† x·ª≠ l√Ω Checklist (XLSX) t·ª´ Google Drive...")
def load_checklist(file_id, filename, access_token):
    """ T·∫£i checklist XLSX v√† ƒë·ªçc th√†nh DataFrame. """
    
    if not os.path.exists(filename):
        # Truy·ªÅn token v√†o h√†m download
        download_file_from_gdrive(file_id, filename, access_token)
        
    if os.path.exists(filename):
        try:
            # ƒê·ªåC FILE XLSX
            df = pd.read_excel(filename) 
            return df
        except Exception as e:
            st.error(f"‚ùå L·ªói khi ƒë·ªçc file checklist: {e}. ƒê·∫£m b·∫£o file c√≥ ƒë·ªãnh d·∫°ng XLSX.")
            return None
    return None

@st.cache_resource
def load_face_cascade(url, filename):
    """ T·∫£i Haar Cascade cho OpenCV. """
    try:
        r = requests.get(url)
        if r.status_code == 200:
            with open(filename, 'wb') as f:
                f.write(r.content)
            classifier = cv2.CascadeClassifier(filename)
            if not classifier.empty():
                return classifier
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i ho·∫∑c kh·ªüi t·∫°o Haar Cascade: {e}")
        return None

face_cascade = load_face_cascade(HAAR_CASCADE_URL, CASCADE_FILENAME)

# --- 3. H√†m Ph√°t hi·ªán Khu√¥n m·∫∑t (Gi·ªØ nguy√™n) ---
def detect_and_draw_face(image_bytes, cascade):
    """ D√πng Haar Cascade ƒë·ªÉ ph√°t hi·ªán v√† v·∫Ω khung khu√¥n m·∫∑t tr√™n ·∫£nh. """
    image_pil = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    image_np = np.array(image_pil)
    image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    
    faces = []
    if cascade is not None:
        faces = cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    for (x, y, w, h) in faces:
        cv2.rectangle(image_bgr, (x, y), (x + w, y + h), (255, 0, 0), 2)
    
    processed_image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

    return processed_image_rgb, len(faces) > 0, len(faces), image_bgr

# --- 4. H√†m DeepFace Recognition (Gi·ªØ nguy√™n) ---
def verify_face_against_dataset(target_image_path, dataset_folder):
    """ S·ª≠ d·ª•ng DeepFace ƒë·ªÉ so s√°nh ·∫£nh ƒë·∫ßu v√†o v·ªõi dataset. """
    try:
        df_list = DeepFace.find(
            img_path=target_image_path, 
            db_path=dataset_folder, 
            model_name="ArcFace",
            distance_metric="cosine",
            enforce_detection=True, 
            detector_backend=DETECTOR_BACKEND 
        )
        if isinstance(df_list, list) and len(df_list) > 0 and not df_list[0].empty:
            best_match = df_list[0].iloc[0]
            identity_path = best_match['identity']
            stt_match = os.path.splitext(os.path.basename(identity_path))[0] 
            distance = best_match['ArcFace_cosine'] 
            return stt_match, distance
        return None, None
    except Exception as e:
        if "Face could not be detected" in str(e):
             st.error(f"‚ùå L·ªói DeepFace: Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t ƒë·ªÉ so kh·ªõp.")
        else:
            st.error(f"‚ùå L·ªói DeepFace: {e}")
        return None, None

# --- 5. H√†m MOCK UPLOAD l√™n Google Drive ---
def upload_to_gdrive_mock(file_path, drive_folder_id, drive_filename, access_token):
    """
    [MOCK/PLACEHOLDER] H√†m gi·∫£ ƒë·ªãnh vi·ªác t·∫£i file l√™n Google Drive, s·ª≠ d·ª•ng token.
    """
    if access_token is None:
        st.error("‚ùå L·ªói Auth: Kh√¥ng th·ªÉ upload v√¨ kh√¥ng c√≥ Access Token h·ª£p l·ªá.")
        return False
    
    st.success(f"‚úÖ **M√¥ ph·ªèng Upload:** T·∫£i file '{drive_filename}' th√†nh c√¥ng.")
    st.info(f"ƒê√£ gi·∫£ l·∫≠p l∆∞u v√†o Drive Folder ID: **{drive_folder_id}** b·∫±ng Access Token.")
    
    return True

# --- 6. Logic Ghi D·ªØ Li·ªáu v√† L∆∞u ·∫¢nh M·ªõi ---

def update_checklist_and_save_new_data(stt_match, session_name, image_bytes, access_token):
    """
    C·∫≠p nh·∫≠t DataFrame checklist v√† l∆∞u ·∫£nh m·ªõi l√™n Drive.
    """
    if CHECKLIST_SESSION_KEY not in st.session_state:
        st.error("L·ªói: Kh√¥ng t√¨m th·∫•y DataFrame checklist trong Session State.")
        return

    df = st.session_state[CHECKLIST_SESSION_KEY]
    
    # 1. C·∫≠p nh·∫≠t Checklist (ƒê√°nh 'X')
    if stt_match is not None:
        try:
            stt_col = df.columns[0] 
            stt_match_str = str(stt_match).split('_')[0] 
            
            row_index = df[df[stt_col].astype(str).str.contains(stt_match_str, regex=False)].index
            
            if not row_index.empty:
                df.loc[row_index[0], session_name] = 'X'
                st.session_state[CHECKLIST_SESSION_KEY] = df 
                
                st.success(f"‚úÖ **ƒê√£ c·∫≠p nh·∫≠t ƒëi·ªÉm danh** cho STT **{df.loc[row_index[0], stt_col]}** v√†o c·ªôt **{session_name}**.")
                st.info(f"‚ö†Ô∏è **M√¥ ph·ªèng:** DataFrame n√†y c·∫ßn ƒë∆∞·ª£c ghi tr·ªü l·∫°i file XLSX Drive ID: **{GDRIVE_CHECKLIST_ID}** b·∫±ng Access Token.")
                
            else:
                st.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y STT **{stt_match_str}** trong checklist ƒë·ªÉ c·∫≠p nh·∫≠t.")
        except Exception as e:
            st.error(f"L·ªói khi c·∫≠p nh·∫≠t checklist: {e}")
            
    # 2. L∆∞u ·∫£nh m·ªõi l√™n Drive (N·∫øu kh√¥ng kh·ªõp)
    else: 
        if 'new_data_counter' not in st.session_state:
            st.session_state['new_data_counter'] = 0
            
        st.session_state['new_data_counter'] += 1
        new_counter = st.session_state['new_data_counter']
        
        session_num = session_name.replace("Bu·ªïi ", "")
        drive_filename = f"B{session_num}_{new_counter}.jpg" 
        
        # --- T·∫†O FILE T·∫†M ƒê·ªÇ UPLOAD ---
        temp_file_for_upload = tempfile.NamedTemporaryFile(suffix=".jpg", delete=False)
        TEMP_UPLOAD_PATH = temp_file_for_upload.name
        temp_file_for_upload.close()
        
        try:
            image_to_save = Image.open(io.BytesIO(image_bytes)).convert('RGB')
            image_to_save.save(TEMP_UPLOAD_PATH, format='JPEG')
            
            # 2. G·ªçi h√†m Upload Drive (MOCK) v√† truy·ªÅn token
            upload_to_gdrive_mock(TEMP_UPLOAD_PATH, GDRIVE_NEW_DATA_FOLDER_ID, drive_filename, access_token)

        except Exception as e:
             st.error(f"‚ùå L·ªói khi t·∫°o file t·∫°m ho·∫∑c g·ªçi h√†m upload: {e}")
        finally:
            if os.path.exists(TEMP_UPLOAD_PATH):
                os.remove(TEMP_UPLOAD_PATH)


# --- 7. Giao di·ªán v√† Lu·ªìng ·ª®ng d·ª•ng ---
# L·∫§Y TOKEN ƒê·∫¶U TI√äN
ACCESS_TOKEN = get_valid_access_token_mock(GDRIVE_CLIENT_ID, GDRIVE_CLIENT_SECRET)

if not ACCESS_TOKEN:
    st.error("‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c do kh√¥ng l·∫•y ƒë∆∞·ª£c Access Token h·ª£p l·ªá t·ª´ quy tr√¨nh OAuth gi·∫£ l·∫≠p.")
    st.stop()


# 7.1 KH·ªûI T·∫†O V√Ä T·∫¢I DATASET & CHECKLIST
# T·∫£i Folder Dataset (MOCK)
dataset_ready = download_dataset_folder_mock(GDRIVE_DATASET_FOLDER_ID, DATASET_FOLDER, ACCESS_TOKEN) 
# T·∫£i Checklist (XLSX)
checklist_df = load_checklist(GDRIVE_CHECKLIST_ID, CHECKLIST_FILENAME, ACCESS_TOKEN)

if checklist_df is not None:
    st.session_state[CHECKLIST_SESSION_KEY] = checklist_df
    
st.markdown("---")

if not dataset_ready:
     st.warning("‚ö†Ô∏è L·ªói m√¥ ph·ªèng t·∫£i Dataset Folder. Vui l√≤ng ki·ªÉm tra ID Drive Folder v√† quy·ªÅn truy c·∫≠p.")
     st.stop()
     
if checklist_df is None:
     st.warning("‚ö†Ô∏è L·ªói t·∫£i ho·∫∑c ƒë·ªçc file Checklist. Vui l√≤ng ki·ªÉm tra File ID v√† quy·ªÅn truy c·∫≠p b·∫±ng token.")
     st.stop()


st.info(f"Dataset ƒë√£ s·∫µn s√†ng. Checklist c√≥ {len(checklist_df)} ng∆∞·ªùi.")


# 7.2 CH·ªåN BU·ªîI H·ªåC (Dropdown)
attendance_cols = [col for col in st.session_state[CHECKLIST_SESSION_KEY].columns if "Bu·ªïi" in col]

if not attendance_cols:
     st.error("Kh√¥ng t√¨m th·∫•y c·ªôt 'Bu·ªïi' trong checklist. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u tr√∫c file XLSX.")
     st.stop()

selected_session = st.selectbox(
    "1Ô∏è‚É£ **Ch·ªçn Bu·ªïi ƒêi·ªÉm Danh**", 
    attendance_cols, 
    index=0,
    help="Ch·ªçn bu·ªïi t∆∞∆°ng ·ª©ng ƒë·ªÉ c·∫≠p nh·∫≠t c·ªôt ƒëi·ªÉm danh trong checklist."
)
st.success(f"ƒêang ƒëi·ªÉm danh cho: **{selected_session}**")

st.markdown("---")

# 7.3 CH·ª§P ·∫¢NH V√Ä X·ª¨ L√ù
captured_file = st.camera_input("2Ô∏è‚É£ Ch·ª•p ·∫£nh ƒëi·ªÉm danh:")

if captured_file is not None:
    
    image_bytes = captured_file.getvalue()
    
    with st.spinner('ƒêang x·ª≠ l√Ω ·∫£nh v√† nh·∫≠n di·ªán khu√¥n m·∫∑t...'):
        
        processed_image_np, face_detected, num_faces, image_bgr = detect_and_draw_face(image_bytes, face_cascade)
        processed_image = Image.fromarray(processed_image_np)
        
        # L∆ØU ·∫¢NH T·∫†M TH·ªúI cho DeepFace so kh·ªõp
        temp_file = tempfile.NamedTemporaryFile(suffix=".jpg", delete=False)
        TEMP_IMAGE_PATH = temp_file.name
        temp_file.close() 
        
        cv2.imwrite(TEMP_IMAGE_PATH, image_bgr)
        
        # Th·ª±c hi·ªán so kh·ªõp DeepFace
        stt_match, distance = verify_face_against_dataset(TEMP_IMAGE_PATH, DATASET_FOLDER)

    # X√≥a file t·∫°m c·ªßa DeepFace
    if os.path.exists(TEMP_IMAGE_PATH):
        os.remove(TEMP_IMAGE_PATH)
        
    st.subheader("üñºÔ∏è ·∫¢nh ƒë√£ ch·ª•p v√† Nh·∫≠n di·ªán")
    st.image(processed_image, caption="Khu√¥n m·∫∑t ƒë√£ ph√°t hi·ªán ƒë∆∞·ª£c ƒë√°nh d·∫•u.", use_column_width=True)

    st.markdown("---")
    st.subheader("üí° K·∫øt qu·∫£ ƒêi·ªÉm danh")
    
    if stt_match:
        st.balloons()
        st.success(f"‚úÖ **ƒêI·ªÇM DANH TH√ÄNH C√îNG!**")
        st.markdown(f"""
        * **STT tr√πng kh·ªõp:** **{stt_match}**
        * **ƒê·ªô t∆∞∆°ng ƒë·ªìng (Kho·∫£ng c√°ch Cosine):** `{distance:.4f}`
        """)
        # C·∫≠p nh·∫≠t checklist (truy·ªÅn token)
        update_checklist_and_save_new_data(stt_match, selected_session, None, ACCESS_TOKEN)
        
    elif face_detected and num_faces == 1:
        st.warning(f"‚ö†Ô∏è **Ph√°t hi·ªán 1 khu√¥n m·∫∑t, nh∆∞ng kh√¥ng kh·ªõp v·ªõi dataset.**")
        # L∆∞u ·∫£nh m·ªõi (truy·ªÅn image_bytes v√† token)
        update_checklist_and_save_new_data(None, selected_session, image_bytes, ACCESS_TOKEN) 
        
    elif face_detected and num_faces > 1:
        st.error(f"‚ùå **Ph√°t hi·ªán nhi·ªÅu khu√¥n m·∫∑t ({num_faces}). Vui l√≤ng ch·ªâ c√≥ 1 ng∆∞·ªùi trong khung h√¨nh.**")

    else:
        st.warning("‚ö†Ô∏è **Kh√¥ng ph√°t hi·ªán th·∫•y khu√¥n m·∫∑t.**")
        st.markdown("Vui l√≤ng th·ª≠ l·∫°i. ƒê·∫£m b·∫£o khu√¥n m·∫∑t c·ªßa b·∫°n n·∫±m g·ªçn v√† r√µ r√†ng trong khung h√¨nh.")

st.markdown("---")
st.subheader("üìã Tr·∫°ng th√°i Checklist Hi·ªán t·∫°i (Trong Session)")
if CHECKLIST_SESSION_KEY in st.session_state:
    st.dataframe(st.session_state[CHECKLIST_SESSION_KEY])
